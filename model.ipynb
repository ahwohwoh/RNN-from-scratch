  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 2 APRIL 2018 (MON) 17:00PM**\n",
    "\n",
    "In this assignment we will be coding the building blocks for the Recurrent Neural Network (RNN) in `rnn_layers.py` and putting them together to train a RNN on sentiment analysis. \n",
    "\n",
    "**Attention: Only python3 will be allowed to use in this assignment. And we use numpy to store and caculate data and parameters. You do not need a GPU for this assignment. CPU is enough. To run this Jupyter notebook, you need to install the depedent libraries in [requiremets.txt](requirements.txt) via pip (or pip3). Note: If you don't implement all the codes, running the codes might occur some errors.**\n",
    "\n",
    "For each layer we will implement a forward and a backward function. The forward function will receive inputs and will return the outputs of this layer(loss layer will be a little different), and the backward pass will receive upstream derivatives and inputs and will return gradients with respect to the inputs. Gradients for weights or bias will be stored in parameters in this layer:\n",
    "\n",
    "```python\n",
    "class SomeLayer(Layer):\n",
    "    # some layer type inherited from Layer class\n",
    "    def __init__(self, params):\n",
    "        # set up specific layer parameters\n",
    "        # initialize variables for the layer weights\n",
    "        # initialize variables for storing the gradients\n",
    "        # initialize other necessary variables\n",
    "    def forward(self, inputs):\n",
    "        # Receive inputs and return output\n",
    "        # Do some computations ...\n",
    "        z = # ... some intermediate value\n",
    "        # Do some more computations ...\n",
    "        outputs = # the outputs\n",
    "        return outputs\n",
    "    def backward(self, in_grads, inputs):\n",
    "        # Receive derivative of loss with respect to outputs,\n",
    "        # and compute derivative with respect to inputs.\n",
    "        # Use values in cache to compute derivatives\n",
    "        out_grads = # Derivative of loss with respect to inputs\n",
    "        self.w_grad = # Derivative of loss with respect to self.weights\n",
    "        return out_grads\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers (i.e. `RNN Cell`, `RNN`, `Bidirectional RNN`) in this way, we will be able to easily combine them to build classifiers for various applications whose input are sequential data (e.g. Sentiment Analysis).\n",
    "\n",
    "This iPython notebook serves to:\n",
    "- explain the questions\n",
    "- explain the function APIs and implementation examples\n",
    "- provide helper functions to piece functions together and check your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Cell Layer\n",
    "\n",
    "RNN cell is the basic building block of RNN, which implements the specific operation at each time step of RNN. It has an hidden states of dimension `H` and accepts inputs of dimension `D`. In this assignment, you are required to implement a simple type of RNN cell, formulated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "y=tanh(Wx+Uh+b),\n",
    "\\end{equation*}\n",
    "\n",
    "where `x` and `h` are the inputs and hidden states respectively, and `W`, `U` and `b` are trainable kernel, recurrent_kernel and bias respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "\n",
    "Please implement the function `RNNCell.forward(self, inputs)` and test your implementation using the following code. (`inputs` is a list of two numpy arrays, `[x, h]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell\n",
    "from utils.tools import rel_error\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.random.uniform(size=(N, D))\n",
    "prev_h = np.random.uniform(size=(N, H))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "out = rnn_cell.forward([x, prev_h])\n",
    "# compare with the keras implementation\n",
    "keras_x = layers.Input(shape=(1, D), name='x')\n",
    "keras_prev_h = layers.Input(shape=(H,), name='prev_h')\n",
    "keras_rnn = layers.RNN(layers.SimpleRNNCell(H),\n",
    "                       name='rnn')(keras_x, initial_state=keras_prev_h)\n",
    "keras_model = keras.Model(inputs=[keras_x, keras_prev_h], \n",
    "                          outputs=keras_rnn)\n",
    "keras_model.get_layer('rnn').set_weights([rnn_cell.kernel,\n",
    "                                          rnn_cell.recurrent_kernel,\n",
    "                                          rnn_cell.bias])\n",
    "keras_out = keras_model.predict_on_batch([x[:, None, :], prev_h])\n",
    "\n",
    "print('Relative error (<1e-5 will be fine): {}'.format(rel_error(keras_out, out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "\n",
    "Please implement the function `RNNCell.backward(self, in_grads, inputs)` and test your implementation using the following code. You need to compute the gradients to both the inputs and hidden states, as well as those trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.random.uniform(size=(N, D))\n",
    "prev_h = np.random.uniform(size=(N, H))\n",
    "in_grads = np.random.uniform(size=(N, H))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "check_grads_layer(rnn_cell, [x, prev_h], in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then please improve your implementation of RNN cell so that it can properly handle `NaN` input, and test it with the following code. **The gradients to those `NaN` input units are supposed to be zeros.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.random.uniform(size=(N, D))\n",
    "# set part of input to NaN\n",
    "# this situation will be encountered in the following work\n",
    "x[1:, :] = np.nan\n",
    "prev_h = np.random.uniform(size=(N, H))\n",
    "in_grads = np.random.uniform(size=(N, H))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "check_grads_layer(rnn_cell, [x, prev_h], in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Layer\n",
    "\n",
    "RNN layer wraps any type of RNN cell so that it can operate over a sequence of input data of different length. In particular, it runs a instance of RNN cell over the inputs, holds and updates the hidden states for the RNN cell. In this assignment, you are required to implement such a general RNN layer that is able to wrap your implemented RNN cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "\n",
    "Please implement the function `RNN.forward(self, inputs)` and test your implementation using the following code. Since NN layers generally proceed on a batch of data simultaneously, and for RNN, each input data may have different length, we define the input data format as an array of `(N, T, D)`, where `N` is the number of samples in a batch, `T` is the maximum length of input sequences, and `D` is the dimension of features at each time step. `NaN` is used to pad input sequences of different lenghts, so that the resulting length equals to `T`, e.g. `(x1, x2, ..., xk, NaN, NaN)`. **Hint: you can utilze `np.nan_to_num(x)` to easily convert NaNs to zeros in a numpy array, and `np.isnan(x)` to get binary mask indicating which elements are NaNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell, RNN\n",
    "from utils.tools import rel_error\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.random.uniform(size=(N, T, D))\n",
    "x[0, -1:, :] = np.nan\n",
    "x[1, -2:, :] = np.nan\n",
    "h0 = np.random.uniform(size=(H,))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "rnn = RNN(rnn_cell, h0=h0)\n",
    "out = rnn.forward(x)\n",
    "\n",
    "keras_x = layers.Input(shape=(T, D), name='x')\n",
    "keras_h0 = layers.Input(shape=(H,), name='h0')\n",
    "keras_rnn = layers.RNN(layers.SimpleRNNCell(H), return_sequences=True,\n",
    "                       name='rnn')(keras_x, initial_state=keras_h0)\n",
    "keras_model = keras.Model(inputs=[keras_x, keras_h0],\n",
    "                          outputs=keras_rnn)\n",
    "keras_model.get_layer('rnn').set_weights([rnn.kernel,\n",
    "                                          rnn.recurrent_kernel,\n",
    "                                          rnn.bias])\n",
    "keras_out = keras_model.predict_on_batch([x, np.tile(h0, (N, 1))])\n",
    "\n",
    "print('Relative error (<1e-5 will be fine): {}'.format(rel_error(keras_out, out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "\n",
    "Please implement the function `RNN.backward(self, in_grads, inputs)` and test your implementation using the following code (**note the internal gradients passed from next time steps**). Once again: the gradients to those `NaN` input units are supposed to be zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell, RNN\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.random.uniform(size=(N, T, D))\n",
    "x[0, -1:, :] = np.nan\n",
    "x[1, -2:, :] = np.nan\n",
    "in_grads = np.random.uniform(size=(N, T, H))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "rnn = RNN(rnn_cell)\n",
    "check_grads_layer(rnn, x, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional RNN Layer\n",
    "\n",
    "Vallina RNN operates over input sequence in one direction, so it has limitations as the future input information cannot be reached from the current state. On the contrary, Bi-directional RNN addresses this shortcoming by operating the input sequence in both forward and backward directions. \n",
    "\n",
    "Usually, Bi-directional RNN is implemented by running two independent RNNs in opposite direction of input data, and concatenating the outputs of the two RNNs. Since you have implemented RNN layer above, implementing Bi-directional RNN layer is not hard, which just requires certain manipulation of input data. A useful function that can reverse a batch of sequence data is provided for your easy implementation.\n",
    "\n",
    "```python\n",
    "def _reverse_temporal_data(self, x, mask):\n",
    "    num_nan = np.sum(~mask, axis=1)\n",
    "    reversed_x = np.array(x[:, ::-1, :])\n",
    "    for i in range(num_nan.size):\n",
    "        reversed_x[i] = np.roll(reversed_x[i], x.shape[1]-num_nan[i], axis=0)\n",
    "    return reversed_x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "\n",
    "We provided the function `BidirectionalRNN.forward(self, inputs)` and the following code for testing. Note that `H` is the dimension of the hidden states of one internal RNN, so the actual dimension of the hidden states (or outputs) of Bidirectional RNN is `2*H`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell, BidirectionalRNN\n",
    "from utils.tools import rel_error\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.random.uniform(size=(N, T, D))\n",
    "x[0, -1:, :] = np.nan\n",
    "x[1, -2:, :] = np.nan\n",
    "h0 = np.random.uniform(size=(N, H))\n",
    "hr = np.random.uniform(size=(N, H))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "brnn = BidirectionalRNN(rnn_cell, h0=h0, hr=hr)\n",
    "out = brnn.forward(x)\n",
    "\n",
    "keras_x = layers.Input(shape=(T, D), name='x')\n",
    "keras_h0 = layers.Input(shape=(H,), name='h0')\n",
    "keras_hr = layers.Input(shape=(H,), name='hr')\n",
    "keras_x_masked = layers.Masking(mask_value=0.)(keras_x)\n",
    "keras_rnn = layers.RNN(layers.SimpleRNNCell(H), return_sequences=True)\n",
    "keras_brnn = layers.Bidirectional(keras_rnn, merge_mode='concat', name='brnn')(\n",
    "        keras_x_masked, initial_state=[keras_h0, keras_hr])\n",
    "keras_model = keras.Model(inputs=[keras_x, keras_h0, keras_hr],\n",
    "                          outputs=keras_brnn)\n",
    "keras_model.get_layer('brnn').set_weights([brnn.forward_rnn.kernel,\n",
    "                                           brnn.forward_rnn.recurrent_kernel, \n",
    "                                           brnn.forward_rnn.bias,\n",
    "                                           brnn.backward_rnn.kernel, \n",
    "                                           brnn.backward_rnn.recurrent_kernel,\n",
    "                                           brnn.backward_rnn.bias])\n",
    "keras_out = keras_model.predict_on_batch([np.nan_to_num(x), h0, hr])\n",
    "nan_indices = np.where(np.any(np.isnan(x), axis=2))\n",
    "keras_out[nan_indices[0], nan_indices[1], :] = np.nan\n",
    "\n",
    "print('Relative error (<1e-5 will be fine): {}'.format(rel_error(keras_out, out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "\n",
    "Please refer to the provided forward function and implement the function `BidirectionalRNN.backward(self, inputs)`. Test your implementation using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import rnn_layers\n",
    "importlib.reload(rnn_layers)\n",
    "from rnn_layers import RNNCell, BidirectionalRNN\n",
    "from utils.check_grads import check_grads_layer\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.random.uniform(size=(N, T, D))\n",
    "x[0, -1:, :] = np.nan\n",
    "x[1, -2:, :] = np.nan\n",
    "in_grads = np.random.uniform(size=(N, T, H*2))\n",
    "\n",
    "rnn_cell = RNNCell(in_features=D, units=H)\n",
    "brnn = BidirectionalRNN(rnn_cell)\n",
    "check_grads_layer(brnn, x, in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using RNNs\n",
    "\n",
    "In this section, you are required to test your implementations above by running an ensemble NN on a sentiment analysis dataset. The dataset, `data/corpus.csv`, consists of 800 real movie comments and the corresponding labels that indicate whether the comments are positive or negative. For example:\n",
    "```\n",
    "POSTIVE: I absolutely LOVE Harry Potter, as you can tell already.\n",
    "NEGATIVE: My dad's being stupid about brokeback mountain...\n",
    "```\n",
    "\n",
    "We provide a basic NN for your experiments, which can be found in `applications.py`. The architecture is as follow:\n",
    "```python\n",
    "FCLayer(vocab_size, 200, name='embedding')\n",
    "BidirectionalRNN(RNNCell(in_features=200, units=50))\n",
    "FCLayer(100, 32, name='fclayer1')\n",
    "TemporalPooling()\n",
    "FCLayer(32, 2, name='fclayer2')\n",
    "```\n",
    "The input to the network is sequences of one-hot vectors, each of which represents a word. The 1st FC layer works as an [embedding layer](https://www.tensorflow.org/versions/master/programmers_guide/embedding) to learn and retrieve the word embedding vectors. After a Bi-directional RNN layer and another FC layer, a TemporalPooling layer (see `layers.py`) is used to mean-pooling a sequence of vectors into one vector, which will ignore the filling `NaN`s. The rest of the network is same as a normal NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xingru.chen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Number of training samples: 600\n",
      "Number of validation samples: 100\n",
      "Number of testing samples: 100\n",
      "vocab_size 763\n",
      "train_loader <generator object Sentiment.train_loader at 0x1a17a42ca8>\n",
      "Epoch 0: \n",
      "inputs (100, 30, 200)\n",
      "np.isnan(inputs) (100, 30, 200)\n",
      "Test accuracy=0.47000, loss=0.69315\n",
      "inputs (100, 30, 200)\n",
      "np.isnan(inputs) (100, 30, 200)\n",
      "Validation accuracy: 0.44000, loss: 0.69315\n",
      "self._one_hot_encoding(self.x_train[idx]) (20, 30, 763)\n",
      "self.y_train[idx].shape (20,)\n",
      "inputs (20, 30, 200)\n",
      "np.isnan(inputs) (20, 30, 200)\n",
      "Iteration 0:\taccuracy=0.35000, loss=0.69315, regularization loss= 0.00903604620936\n",
      "self._one_hot_encoding(self.x_train[idx]) (20, 30, 763)\n",
      "self.y_train[idx].shape (20,)\n",
      "inputs (20, 30, 200)\n",
      "np.isnan(inputs) (20, 30, 200)\n",
      "self._one_hot_encoding(self.x_train[idx]) (20, 30, 763)\n",
      "self.y_train[idx].shape (20,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc0d798695a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         val_intervals=100, test_intervals=100, print_intervals=5)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/NUS_CS/2017:2018 sem2/DL CS5242 NEURAL NETWORKS AND DEEP LEARNING/assignment2/codes/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, train_batch, val_batch, test_batch, epochs, val_intervals, test_intervals, print_intervals)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mtrain_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NUS_CS/2017:2018 sem2/DL CS5242 NEURAL NETWORKS AND DEEP LEARNING/assignment2/codes/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mlayer_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NUS_CS/2017:2018 sem2/DL CS5242 NEURAL NETWORKS AND DEEP LEARNING/assignment2/codes/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mb_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_reshaped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import datasets\n",
    "from applications import SentimentNet\n",
    "from loss import SoftmaxCrossEntropy, L2\n",
    "from optimizers import Adam, Adagrad, RMSprop\n",
    "import numpy as np\n",
    "np.random.seed(5242)\n",
    "\n",
    "dataset = datasets.Sentiment()\n",
    "model = SentimentNet(dataset.dictionary)\n",
    "loss = SoftmaxCrossEntropy(num_class=2)\n",
    "\n",
    "adam = Adam(lr=0.001, decay=0,\n",
    "            scheduler_func=lambda lr, it: lr*0.5 if it%1000==0 else lr)\n",
    "Adagrad = Adagrad(lr=0.01, decay=0)\n",
    "RMSprop = RMSprop()\n",
    "model.compile(optimizer=Adagrad, loss=loss, regularization=L2(w=0.001))\n",
    "train_results, val_results, test_results = model.train(\n",
    "        dataset, \n",
    "        train_batch=20, val_batch=100, test_batch=100, \n",
    "        epochs=15, \n",
    "        val_intervals=100, test_intervals=100, print_intervals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(2, figsize=(18, 8))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.plot(val_results[:,0], val_results[:,1])\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('Validation accuracy')\n",
    "plt.plot(val_results[:,0], val_results[:,2])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your best Sentiment Analysis net\n",
    "Based on the provided NN, tweak the hyperparameters and use what you've learnt to train the best net on sentiment analysis (you should not need to wait half a day for the training to complete). You are free to use more features, hidden units, layers etc. In your report, please write the following:\n",
    "- Training and test accuracy over iterations\n",
    "- Architecture and training method (eg. optimization scheme, data augmentation): explain your design choices, what has failed and what has worked and why you think they worked/failed\n",
    "\n",
    "Credits will be given based on your test accuracy and explanation on your network architecture and training method. Large component of the grading will subject to the latter. Use only the code you have written and any helper functions provided in this assignment and **assignment 1**. Do not use external libraries like Tensorflow and Pytorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
